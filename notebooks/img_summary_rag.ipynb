{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../../python'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from cvpr2018.feature_extractor import get_features_loader\n",
    "from cvpr2018.utils.utils import register_logger\n",
    "from encoder.clip_encoder import ClipEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "log_every = 50  # log the writing of clips every n steps\n",
    "log_file = None  # set logging file\n",
    "num_workers = 4  # define the number of workers used for loading the videos\n",
    "\n",
    "cudnn.benchmark = True\n",
    "register_logger(log_file=log_file)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataset_path = '/home/ubuntu/repos/llm-rag/data/Anomaly-Videos-Part-1/test'  # path to the video dataset\n",
    "clip_length = 16  # define the length of each input sample\n",
    "frame_interval = 1 # define the sampling interval between framesq\n",
    "batch_size = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "llm.invoke(\"how are you doing today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import numpy as np\n",
    "from lavis.models import load_model_and_preprocess\n",
    "from torchvision.transforms import ToPILImage\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenCLIPEmbeddingFunction\n",
    "from chromadb.utils.data_loaders import ImageLoader\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "class ClipEncoder:\n",
    "    def __init__(self, dataset_path, clip_length, caption_model_type, frame_interval, batch_size, num_workers):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.dataset_path = dataset_path\n",
    "        self.clip_length = clip_length\n",
    "        self.frame_interval = frame_interval\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.chroma_client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "        self.data_loader, self.data_iter = get_features_loader(dataset_path,\n",
    "                                                                    clip_length,\n",
    "                                                                    frame_interval,\n",
    "                                                                    batch_size,\n",
    "                                                                    num_workers,\n",
    "                                                                    \"clip\"\n",
    "                                                                    )\n",
    "        self.model, self.preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "        self.caption_model, self.vis_processors, _ = load_model_and_preprocess(name=\"blip_caption\",\n",
    "                                                                               model_type=caption_model_type,\n",
    "                                                                               is_eval=True,\n",
    "                                                                               device=device)\n",
    "    def encode_image(self, idx):\n",
    "        frame_tensor = self.data_loader[idx][0].permute(1, 0, 2, 3)\n",
    "        with torch.no_grad():\n",
    "            frame_embeddings = self.model.encode_image(frame_tensor.cuda())\n",
    "        return frame_embeddings\n",
    "\n",
    "    def get_all_image_embeddings(self):\n",
    "        embeddings = []\n",
    "        for idx in range(len(self.data_loader)):\n",
    "            emb = self.encode_image(idx)\n",
    "            embeddings.append(emb)\n",
    "        return embeddings\n",
    "    \n",
    "    def export_tensor_to_np(self):\n",
    "        arr = []\n",
    "        for idx in range(len(self.data_loader)):\n",
    "            frame_tensor = self.data_loader[idx][0].permute(1, 0, 2, 3)\n",
    "            pil_image = ToPILImage()(frame_tensor[0]) \n",
    "            arr.append(np.array(pil_image))\n",
    "        return arr\n",
    "    \n",
    "    def export_tensor_to_base64(self):\n",
    "        arr = []\n",
    "        for idx in range(len(self.data_loader)):\n",
    "            frame_tensor = self.data_loader[idx][0].permute(1, 0, 2, 3)\n",
    "            pil_image = ToPILImage()(frame_tensor[0])\n",
    "            buffered = BytesIO()\n",
    "            pil_image.save(buffered, format=\"JPEG\")\n",
    "            img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "            arr.append(img_str)\n",
    "        return arr\n",
    "    \n",
    "    def get_captions(self):\n",
    "        captions_list = []  \n",
    "        for idx in range(len(self.data_loader)):  \n",
    "            frame_tensor = self.data_loader[idx][0].permute(1, 0, 2, 3)\n",
    "            pil_image = ToPILImage()(frame_tensor[0]) \n",
    "            image = self.vis_processors[\"eval\"](pil_image).unsqueeze(0).to(self.device)\n",
    "            generated_captions = self.caption_model.generate({\"image\": image})  \n",
    "            captions_list.append(generated_captions)\n",
    "        return captions_list\n",
    "\n",
    "    def get_all_caption_embeddings(self, captions_list):\n",
    "        # Future improvements: Maybe multiple captions per image; Think about a way how to add anomalous captions / features\n",
    "        caption_embeddings = []\n",
    "        if captions_list:\n",
    "            for caption_set in captions_list:\n",
    "                if caption_set:\n",
    "                    for caption in caption_set:\n",
    "                        if caption and len(caption) > 0:\n",
    "                            with torch.no_grad():\n",
    "                                try:\n",
    "                                    caption_features = clip.tokenize(caption).to(self.device)\n",
    "                                    caption_embedding = self.model.encode_text(caption_features)\n",
    "                                    caption_embeddings.append(caption_embedding)\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Error encoding text for caption: {caption}\")\n",
    "                                    print(f\"Error details: {e}\")\n",
    "        return caption_embeddings\n",
    "    \n",
    "    def generate_document_ids(self):\n",
    "        document_ids = []\n",
    "        for i in range(len(self.data_loader)):\n",
    "            item = self.data_loader.getitem_from_raw_video(idx=i)  \n",
    "            for j in range(self.clip_length):\n",
    "                document_ids.append(str(item[3] + '_' + str(item[1]) + '-' + str(j)))\n",
    "\n",
    "        batched_ids = [document_ids[i:i+clip_length] for i in range(0, len(document_ids), clip_length)]\n",
    "        \n",
    "        return document_ids, batched_ids\n",
    "    \n",
    "    def get_or_create_chroma_collection(self, collection_name, embedding_function=None, data_loader=None):\n",
    "        if embedding_function:\n",
    "            try:\n",
    "                collection = self.chroma_client.get_or_create_collection(name=collection_name, embedding_function=embedding_function, data_loader=data_loader)\n",
    "                return collection\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating collection: {collection_name}\")\n",
    "                print(f\"Error details: {e}\")\n",
    "        else:    \n",
    "            try:\n",
    "                collection = self.chroma_client.get_or_create_collection(collection_name)\n",
    "                return collection\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating collection: {collection_name}\")\n",
    "                print(f\"Error details: {e}\")\n",
    "\n",
    "\n",
    "    def upload_embeddings_to_chroma(self, collection_name, img_data, ids, multi_modal= False, captions=None, documents=None, metadata=None):\n",
    "        if multi_modal:\n",
    "            if not len(img_data) == len(ids):\n",
    "                raise ValueError(\"data and ids must have the same length\")\n",
    "            \n",
    "            embedding_function = OpenCLIPEmbeddingFunction(\"ViT-H-14\",\"laion2b_s32b_b79k\" )\n",
    "            data_loader = ImageLoader()\n",
    "            \n",
    "            collection = self.get_or_create_chroma_collection(collection_name, embedding_function, data_loader)\n",
    "            print(\"Multi Modal Collection created\")\n",
    "\n",
    "            for frame, id_ in zip(img_data, ids):\n",
    "                try:\n",
    "                    collection.add(images=frame[0], metadatas=metadata, ids=id_[0])\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to add document with ID {id_}: {str(e)}\")         \n",
    "        \n",
    "        else:\n",
    "            if not len(img_data) == len(ids):\n",
    "                raise ValueError(\"data and ids must have the same length\")\n",
    "\n",
    "            collection = self.get_or_create_chroma_collection(collection_name)\n",
    "\n",
    "            for emb, id_ in zip(img_data, ids):\n",
    "                try:\n",
    "                    collection.add(documents=documents, embeddings=emb, metadatas=metadata, ids=id_)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to add document with ID {id_}: {str(e)}\")                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "normal_videos = \"/home/ubuntu/repos/llm-rag/data/subset_normal\"\n",
    "anomalous_videos = \"/home/ubuntu/repos/llm-rag/data/subset_anomalous\"\n",
    "normal_encoder = ClipEncoder(normal_videos, clip_length, 'base_coco', frame_interval, batch_size, num_workers)\n",
    "anomalous_encoder = ClipEncoder(anomalous_videos, clip_length, 'base_coco', frame_interval, batch_size, num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal_caption_list = normal_encoder.get_captions()\n",
    "# anomalous_caption_list = anomalous_encoder.get_captions()\n",
    "\n",
    "normal_doc_ids = normal_encoder.generate_document_ids()\n",
    "anomalous_doc_ids = anomalous_encoder.generate_document_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# # Convert lists to dataframes\n",
    "# df_normal = pd.DataFrame(normal_caption_list, columns=['Captions'])\n",
    "# df_anomalous = pd.DataFrame(anomalous_caption_list, columns=['Captions'])\n",
    "\n",
    "# # Write dataframes to CSV\n",
    "# df_normal.to_csv('normal_captions.csv', index=False)\n",
    "# df_anomalous.to_csv('anomalous_captions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv to list of captions\n",
    "\n",
    "df_normal = pd.read_csv('normal_captions.csv')\n",
    "df_anomalous = pd.read_csv('weakly_labeled_anomalous_captions.csv', header=None)\n",
    "\n",
    "normal_caption_list = df_normal['Captions'].tolist()\n",
    "weakly_labeled_anomalous_caption_list = df_anomalous[0].tolist()\n",
    "\n",
    "#weakly_labeled_anomalous_caption_list = [str(anomalous_encoder.data_loader.getitem_from_raw_video(idx=i)[2]) + ' ' + caption for i, caption in enumerate(anomalous_caption_list)]\n",
    "\n",
    "#print(anomalous_encoder.data_loader.getitem_from_raw_video(idx=0)[2] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weakly_labeled_anomalous = pd.DataFrame(weakly_labeled_anomalous_caption_list, columns=['Captions'])\n",
    "df_weakly_labeled_anomalous.to_csv('weakly_labeled_anomalous_captions.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(normal_doc_ids[1]))\n",
    "print(len(weakly_labeled_anomalous_caption_list))\n",
    "\n",
    "print(normal_caption_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First chroma retriever\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "embeddings = OllamaEmbeddings()\n",
    "vectore_store = Chroma(collection_name='text_summary_db', persist_directory='/home/ubuntu/chroma_db/', embedding_function=embeddings)\n",
    "root_path = Path.cwd() / \"data\" / \"doc_store_text_summary\"\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = LocalFileStore(root_path)\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectore_store,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "normal_summary_texts = [\n",
    "    Document(page_content=caption, metadata={id_key: doc_id})\n",
    "    for i, caption in enumerate(normal_caption_list)\n",
    "    for doc_id in normal_doc_ids[1][i]\n",
    "]\n",
    "\n",
    "anomalous_summary_texts = [\n",
    "    Document(page_content=caption, metadata={id_key: doc_id})\n",
    "    for i, caption in enumerate(weakly_labeled_anomalous_caption_list)\n",
    "    for doc_id in anomalous_doc_ids[1][i]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorstore.add_documents(normal_summary_texts)\n",
    "retriever.vectorstore.add_documents(anomalous_summary_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever.docstore.mset(list(zip(normal_doc_ids[0], normal_summary_texts))) # Eventually run llava for long description and add to docstore\n",
    "# retriever.docstore.mset(list(zip(anomalous_doc_ids[0], anomalous_summary_texts))) \n",
    "\n",
    "import pickle\n",
    "\n",
    "retriever.docstore.mset([(doc_id, pickle.dumps(doc)) for doc_id, doc in zip(normal_doc_ids[0], normal_summary_texts)])\n",
    "retriever.docstore.mset([(doc_id, pickle.dumps(doc)) for doc_id, doc in zip(anomalous_doc_ids[0], anomalous_summary_texts)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(\n",
    "    \"traffic\"\n",
    ")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Prompt template\n",
    "template = \"\"\"Answer the question based only on the following context, which are short descriptions of videos:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Option 1: LLM\n",
    "# Option 2: Multi-modal LLM\n",
    "# model = GPT4-V or LLaVA\n",
    "\n",
    "# RAG pipeline\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\n",
    "    \"Which document shows people walking next to lockers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\n",
    "    \"How many videos  show indoor scenes?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normal_encoder.chroma_client.delete_collection('text_summary_db')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomaly_310",
   "language": "python",
   "name": "anomaly_310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
